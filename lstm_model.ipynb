{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import combinations\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def get_config(yaml_name):\n",
    "    conf = OmegaConf.load(yaml_name)\n",
    "    return conf\n",
    "\n",
    "def get_combinations_string(lista,combo):\n",
    "    # Gera as combinações\n",
    "    todas_combinacoes = []\n",
    "    combinacoes_string = [\"#\"]\n",
    "    for r in range(1, combo + 1):\n",
    "        todas_combinacoes.extend(list(combinations(lista, r)))\n",
    "    \n",
    "    for elto in todas_combinacoes:\n",
    "        combinacoes_string.append(\"-\".join(elto))\n",
    "    # Converte as tuplas em strings com underline\n",
    "    return combinacoes_string\n",
    "\n",
    "def mkdir_(destination_folder,name_folders):\n",
    "    for name_dir in name_folders:\n",
    "        try:\n",
    "            os.makedirs(destination_folder+name_dir, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao criar diretório {name_dir}: {str(e)}\")\n",
    "\n",
    "def create_folders():\n",
    "    mkdir_(config['paths']['eval_folder'],combinacoes)\n",
    "    mkdir_(config['paths']['results_folder'],combinacoes)\n",
    "    mkdir_(config['paths']['pred_folder'],combinacoes)\n",
    "\n",
    "lista = ['month', 'dayofweek_num', 'hour', 'holiday', 'bool_weather_missing_values', 'precipType']\n",
    "combinacoes = get_combinations_string(lista,1)\n",
    "\n",
    "config = get_config(\"lstm_config.yaml\")\n",
    "\n",
    "create_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import shutil\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet, RecurrentNetwork\n",
    "from pytorch_forecasting.data import GroupNormalizer, NaNLabelEncoder\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss, MAPE, RMSE\n",
    "\n",
    "import pickle\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_file_list(path):\n",
    "    list_csv = os.listdir(path)\n",
    "    return list_csv\n",
    "\n",
    "def get_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    #estado.drop(columns='Unnamed: 0',inplace=True)\n",
    "    #estado.data = pd.to_datetime(estado.data)\n",
    "    #estado = estado.loc[estado.data >= '2022-01-01']\n",
    "\n",
    "    df['temperature'].fillna(method='ffill', inplace=True)\n",
    "    df['windSpeed'].fillna(method='ffill', inplace=True)\n",
    "    df['year'] = df['year'].astype(str)\n",
    "    df['hour'] = df['hour'].astype(str)\n",
    "    df['month'] = df['month'].astype(str)\n",
    "    df['day'] = df['day'].astype(str)\n",
    "    df['dayofweek_num'] = df['dayofweek_num'].astype(str)\n",
    "    df['house_hold'] = df['house_hold'].astype(str)\n",
    "    df['precipType'] = df['precipType'].astype(str)\n",
    "    df['icon'] = df['icon'].astype(str)\n",
    "    df['holiday'] = df['holiday'].astype(str)\n",
    "    df['summary'] = df['summary'].astype(str)\n",
    "    df['bool_weather_missing_values'] = df['bool_weather_missing_values'].astype(str)\n",
    "\n",
    "    return df \n",
    "\n",
    "def evaluation_metrics(val_dataloader,best_model,householde_name,execution_time):    \n",
    "    predictions = best_model.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "    mae = MAE()(predictions.output, predictions.y)\n",
    "    mape = MAPE()(predictions.output, predictions.y)\n",
    "    smape = SMAPE()(predictions.output, predictions.y)\n",
    "    rmse = RMSE()(predictions.output, predictions.y)\n",
    "\n",
    "    dict_ = {'House_Hold':householde_name.split(\".\")[0].upper(),\n",
    "            'MAE':[mae.to('cpu').numpy().round(3)],\n",
    "            'MAPE':[mape.to('cpu').numpy().round(3)],\n",
    "            'SMAPE':[smape.to('cpu').numpy().round(3)],\n",
    "            'RMSE':[rmse.to('cpu').numpy().round(3)],\n",
    "            'Time_execution':f\"{execution_time:.2f}s\"}\n",
    "\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def get_attention_values(interpretation,householde_name,encoder_list,decoder_list):\n",
    "  att_encoder_values = interpretation['encoder_variables'].to('cpu').numpy()\n",
    "  att_decoder_values = interpretation['decoder_variables'].to('cpu').numpy()\n",
    "  tft_encoder = encoder_list\n",
    "  tft_decoder = decoder_list\n",
    "  encoder_dict = {}\n",
    "  decoder_dict = {}\n",
    "  for value, name in zip(att_encoder_values,tft_encoder):   \n",
    "    encoder_dict[name] = [f\"{np.round(value,4)*100:.2f}%\"]\n",
    "  encoder_dict['House_Hold'] = [householde_name.split(\".\")[0].upper()]\n",
    "  for value, name in zip(att_decoder_values,tft_decoder):   \n",
    "    decoder_dict[name] = [f\"{np.round(value,4)*100:.2f}%\"]\n",
    "  decoder_dict['House_Hold'] = [householde_name.split(\".\")[0].upper()]\n",
    "  return encoder_dict,decoder_dict\n",
    "\n",
    "\n",
    "def cleaning_eval_metrics_results(path_origin, path_destiny,model_name):\n",
    "    list_csv = os.listdir(path_origin)\n",
    "    df_list = []\n",
    "    for csv in list_csv:\n",
    "        if os.path.isfile(os.path.join(path_origin,csv)):\n",
    "            print(csv)\n",
    "            df_list.append(pd.read_csv(path_origin + \"/\" + csv))\n",
    "        else:\n",
    "           print('Nao e csv')\n",
    "    concat_df = pd.concat(df_list)\n",
    "    concat_df.to_csv(path_destiny + \"/\" + f\"{model_name}_metrics_results.csv\",index=False)\n",
    "\n",
    "\n",
    "def cleaning_attention_results(path_origin, path_destiny):\n",
    "    list_csv = os.listdir(path_origin)\n",
    "    df_encoder_list = []\n",
    "    df_decoder_list = []\n",
    "    for csv in list_csv:\n",
    "        if os.path.isfile(os.path.join(path_origin,csv)):\n",
    "            if csv.split(\"_\")[0] == 'decoder':\n",
    "                df_encoder_list.append(pd.read_csv(path_origin + \"/\" + csv))\n",
    "            else:\n",
    "                df_decoder_list.append(pd.read_csv(path_origin + \"/\" + csv))\n",
    "        else:\n",
    "            print('Nao e csv')\n",
    "    concat_df_encoder = pd.concat(df_encoder_list)\n",
    "    concat_df_decoder = pd.concat(df_decoder_list)\n",
    "    concat_df_encoder.to_csv(path_destiny + \"/\" + \"encoder_attention_results.csv\",index=False)\n",
    "    concat_df_decoder.to_csv(path_destiny + \"/\" + \"decoder_attention_results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(yaml_name):\n",
    "    conf = OmegaConf.load(yaml_name)\n",
    "    return conf\n",
    "\n",
    "def get_covariate_combination(path):\n",
    "    \"\"\" \n",
    "    look at eval_metric folder to get all combinations of covariates and return all combinations in a list\n",
    "    \"\"\"\n",
    "    path = config.paths.eval_folder\n",
    "    combinations = os.listdir(path)\n",
    "    feature_combination = {}\n",
    "    for combination in combinations:\n",
    "        feature_combination[combination] = combination.split('-')\n",
    "    return feature_combination\n",
    "\n",
    "\n",
    "config = get_config(\"lstm_config.yaml\")\n",
    "\n",
    "feature_combination = get_covariate_combination(config.paths.eval_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_requires_grad(model):\n",
    "    \"\"\"\n",
    "    Função auxiliar para garantir que todos os parâmetros do modelo tenham requires_grad=True\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    return model\n",
    "\n",
    "def process_batch(batch):\n",
    "    \"\"\"\n",
    "    Processa cada batch para garantir que todos os tensores tenham requires_grad=True\n",
    "    \"\"\"\n",
    "    if isinstance(batch, dict):\n",
    "        return {key: process_batch(value) for key, value in batch.items()}\n",
    "    elif isinstance(batch, list):\n",
    "        return [process_batch(item) for item in batch]\n",
    "    elif isinstance(batch, tuple):\n",
    "        return tuple(process_batch(item) for item in batch)\n",
    "    elif isinstance(batch, torch.Tensor):\n",
    "        return batch.clone().detach().requires_grad_(True)\n",
    "    else:\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RNN_model(df,\n",
    "                  csv_file_name,\n",
    "                  cell_type,\n",
    "                  path_pred,\n",
    "                  path_metrics_val,\n",
    "                  covariates,\n",
    "                  learning_rate = .1,\n",
    "                  hidden_size = 15,\n",
    "                  dropout = .2,\n",
    "                  loss = MAE(),\n",
    "                  optimizer = \"Ranger\",\n",
    "                  rnn_layers  = 2,\n",
    "                  patience=10,\n",
    "                  max_prediction_length = 168,\n",
    "                  max_encoder_length = 720,\n",
    "                  batch_size = 128,\n",
    "                  seed = 81):\n",
    "\n",
    "    df['Energy_kwh'] = df['Energy_kwh'].astype('float32')\n",
    "    df['time_idx'] = df['time_idx'].astype('int32')\n",
    "    training_cutoff = df[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "    pl.seed_everything(seed)\n",
    "\n",
    "    if covariates == [\"time_idx\"]:\n",
    "        training = TimeSeriesDataSet(\n",
    "                                df[lambda x: df.time_idx <= training_cutoff],\n",
    "                                time_idx = 'time_idx',\n",
    "                                target = 'Energy_kwh',\n",
    "                                group_ids = ['house_hold'],\n",
    "                                time_varying_known_reals=['time_idx'],\n",
    "                                time_varying_unknown_reals = ['Energy_kwh'],\n",
    "                                static_categoricals=['house_hold'],\n",
    "                                min_encoder_length = max_encoder_length // 2,\n",
    "                                max_encoder_length = max_encoder_length,\n",
    "                                min_prediction_length=1,\n",
    "                                max_prediction_length = max_prediction_length,\n",
    "                                categorical_encoders = {'house_hold': NaNLabelEncoder(add_nan=True, warn=True)}\n",
    "        )\n",
    "    else:\n",
    "        training = TimeSeriesDataSet(\n",
    "                                    df[lambda x: df.time_idx <= training_cutoff],\n",
    "                                    time_idx = 'time_idx',\n",
    "                                    target = 'Energy_kwh',\n",
    "                                    group_ids = ['house_hold'],\n",
    "                                    #time_varying_known_reals=['time_idx'],\n",
    "                                    time_varying_unknown_reals = ['Energy_kwh'],\n",
    "                                    static_categoricals=['house_hold'],\n",
    "                                    time_varying_known_categoricals = covariates,\n",
    "                                    min_encoder_length = max_encoder_length // 2,\n",
    "                                    max_encoder_length = max_encoder_length,\n",
    "                                    min_prediction_length=1,\n",
    "                                    max_prediction_length = max_prediction_length,\n",
    "                                    categorical_encoders = {'house_hold': NaNLabelEncoder(add_nan=True, warn=True)}\n",
    "        )\n",
    "\n",
    "\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, \n",
    "                                                df,\n",
    "                                                predict = True,\n",
    "                                                stop_randomization = True)\n",
    "\n",
    "\n",
    "    train_dataloader = training.to_dataloader(train = True,\n",
    "                                            batch_size = batch_size,\n",
    "                                            num_workers = 1)\n",
    "\n",
    "\n",
    "    val_dataloader = validation.to_dataloader(train = False,\n",
    "                                            batch_size = batch_size,\n",
    "                                            num_workers = 1)\n",
    "\n",
    "    rnn = RecurrentNetwork.from_dataset(\n",
    "                                            training,\n",
    "                                            cell_type =cell_type,\n",
    "                                            learning_rate = learning_rate,\n",
    "                                            hidden_size = hidden_size,\n",
    "                                            dropout = dropout,\n",
    "                                            loss = loss,\n",
    "                                            optimizer = optimizer,\n",
    "                                            rnn_layers  = rnn_layers\n",
    "        )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor = \"val_loss\",\n",
    "                                    min_delta = 0.0001,\n",
    "                                    patience = patience,\n",
    "                                    verbose = True,\n",
    "                                    mode = \"min\")\n",
    "\n",
    "\n",
    "    lr_logger = LearningRateMonitor()\n",
    "    logger_LSTM = TensorBoardLogger(f\"{cell_type}_logs\")\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\")\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "                            max_epochs = 350,\n",
    "                            accelerator = 'gpu',\n",
    "                            enable_model_summary = True,\n",
    "                            limit_train_batches = 300,\n",
    "                            gradient_clip_val = 0.1,\n",
    "                            callbacks = [lr_logger, early_stop_callback, checkpoint_callback],\n",
    "                            logger = logger_LSTM,\n",
    "                            enable_progress_bar=False\n",
    "            )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    trainer.fit(\n",
    "                    rnn,\n",
    "                    train_dataloaders = train_dataloader,\n",
    "                    val_dataloaders = val_dataloader)\n",
    "    end_time = time.time()  \n",
    "    execution_time = end_time - start_time  \n",
    "\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    best_rnn = RecurrentNetwork.load_from_checkpoint(best_model_path)\n",
    "\n",
    "    best_model_path = str(best_model_path)\n",
    "    best_rnn = RecurrentNetwork.load_from_checkpoint(best_model_path)\n",
    "\n",
    "    predictions = best_rnn.predict(val_dataloader, mode = \"raw\", return_x = True)\n",
    "\n",
    "    df_predictions = pd.DataFrame({'time_idx':predictions.x['decoder_time_idx'][0].to('cpu').numpy(),\n",
    "                                'Real':predictions.x['decoder_target'][0].to('cpu').numpy().round(3),\n",
    "                                'predict':predictions.output[0][0].to('cpu').numpy().round(3).squeeze()})\n",
    "\n",
    "    df_predictions.to_csv(path_pred + csv_file_name, index=False)\n",
    "\n",
    "    eval_dict = evaluation_metrics(val_dataloader,best_rnn,csv_file_name,execution_time)\n",
    "    df_eval_metrics = pd.DataFrame(eval_dict)\n",
    "    df_eval_metrics.to_csv(path_metrics_val + csv_file_name, index=False)\n",
    "    \n",
    "    #shutil.rmtree('lightning_logs')\n",
    "    #shutil.rmtree('LSTM_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder_name,comb_features in feature_combination.items():\n",
    "    if comb_features[0] == \"#\":\n",
    "        comb_features = ['time_idx']\n",
    "    for csv in os.listdir(config.paths.data_dir)[:2]:\n",
    "        print(config.paths.pred_folder+folder_name+'/'+csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(config.paths.data_dir)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(config.paths.data_dir)[20:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new =  {\n",
    " '#': ['#'],\n",
    " #'hour':['hour'],\n",
    " #'precipType': ['precipType'],\n",
    " #'month': ['month'],\n",
    " #'bool_weather_missing_values': ['bool_weather_missing_values'],\n",
    " #'holiday': ['holiday'],\n",
    " #'precipType': ['precipType']\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_problematicos = [\"stretchedSociety_block_91_MAC000350.csv\",\"stretchedSociety_block_107_MAC001785.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_treinados = os.listdir(\"eval_metrics/#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder_name,comb_features in new.items():\n",
    "    if comb_features[0] == \"#\":\n",
    "        comb_features = ['time_idx']\n",
    "    for csv in os.listdir(config.paths.data_dir)[7:]:\n",
    "        if csv in csv_problematicos + csv_treinados:\n",
    "            pass\n",
    "        else:\n",
    "            df = get_csv(config.paths.data_dir + csv)\n",
    "            \n",
    "\n",
    "            print(f\"Training model for {folder_name} using {csv}\")\n",
    "            print(len(df)-168)\n",
    "            run_RNN_model(df,\n",
    "                            max_prediction_length = 168,\n",
    "                            max_encoder_length = len(df)-168,\n",
    "                            covariates = comb_features,\n",
    "                            csv_file_name = csv,\n",
    "                            cell_type = \"LSTM\",\n",
    "                            path_pred = config.paths.pred_folder + folder_name+'/',\n",
    "                            path_metrics_val=config.paths.eval_folder + folder_name+'/')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
