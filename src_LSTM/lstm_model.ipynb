{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import combinations\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def get_config(yaml_name):\n",
    "    conf = OmegaConf.load(yaml_name)\n",
    "    return conf\n",
    "\n",
    "def get_combinations_string(lista,combo):\n",
    "    # Gera as combinações\n",
    "    todas_combinacoes = []\n",
    "    combinacoes_string = [\"#\"]\n",
    "    for r in range(1, combo + 1):\n",
    "        todas_combinacoes.extend(list(combinations(lista, r)))\n",
    "    \n",
    "    for elto in todas_combinacoes:\n",
    "        combinacoes_string.append(\"-\".join(elto))\n",
    "    # Converte as tuplas em strings com underline\n",
    "    return combinacoes_string\n",
    "\n",
    "def mkdir_(destination_folder,name_folders):\n",
    "    for name_dir in name_folders:\n",
    "        try:\n",
    "            os.makedirs(destination_folder+name_dir, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao criar diretório {name_dir}: {str(e)}\")\n",
    "\n",
    "def create_folders():\n",
    "    mkdir_(config['paths']['eval_folder'],combinacoes)\n",
    "    mkdir_(config['paths']['results_folder'],combinacoes)\n",
    "    mkdir_(config['paths']['pred_folder'],combinacoes)\n",
    "\n",
    "lista = ['month', 'dayofweek_num', 'hour', 'holiday', 'bool_weather_missing_values', 'precipType']\n",
    "combinacoes = get_combinations_string(lista,1)\n",
    "\n",
    "config = get_config(\"lstm_config.yaml\")\n",
    "\n",
    "create_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x730f4a197dc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import shutil\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet, RecurrentNetwork\n",
    "from pytorch_forecasting.data import GroupNormalizer, NaNLabelEncoder\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss, MAPE, RMSE\n",
    "\n",
    "import pickle\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_file_list(path):\n",
    "    list_csv = os.listdir(path)\n",
    "    return list_csv\n",
    "\n",
    "def get_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    #estado.drop(columns='Unnamed: 0',inplace=True)\n",
    "    #estado.data = pd.to_datetime(estado.data)\n",
    "    #estado = estado.loc[estado.data >= '2022-01-01']\n",
    "\n",
    "    df['temperature'].fillna(method='ffill', inplace=True)\n",
    "    df['windSpeed'].fillna(method='ffill', inplace=True)\n",
    "    df['year'] = df['year'].astype(str)\n",
    "    df['hour'] = df['hour'].astype(str)\n",
    "    df['month'] = df['month'].astype(str)\n",
    "    df['day'] = df['day'].astype(str)\n",
    "    df['dayofweek_num'] = df['dayofweek_num'].astype(str)\n",
    "    df['house_hold'] = df['house_hold'].astype(str)\n",
    "    df['precipType'] = df['precipType'].astype(str)\n",
    "    df['icon'] = df['icon'].astype(str)\n",
    "    df['holiday'] = df['holiday'].astype(str)\n",
    "    df['summary'] = df['summary'].astype(str)\n",
    "    df['bool_weather_missing_values'] = df['bool_weather_missing_values'].astype(str)\n",
    "\n",
    "    return df \n",
    "\n",
    "def evaluation_metrics(val_dataloader,best_model,householde_name,execution_time):    \n",
    "    predictions = best_model.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "    mae = MAE()(predictions.output, predictions.y)\n",
    "    mape = MAPE()(predictions.output, predictions.y)\n",
    "    smape = SMAPE()(predictions.output, predictions.y)\n",
    "    rmse = RMSE()(predictions.output, predictions.y)\n",
    "\n",
    "    dict_ = {'House_Hold':householde_name.split(\".\")[0].upper(),\n",
    "            'MAE':[mae.to('cpu').numpy().round(3)],\n",
    "            'MAPE':[mape.to('cpu').numpy().round(3)],\n",
    "            'SMAPE':[smape.to('cpu').numpy().round(3)],\n",
    "            'RMSE':[rmse.to('cpu').numpy().round(3)],\n",
    "            'Time_execution':f\"{execution_time:.2f}s\"}\n",
    "\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def get_attention_values(interpretation,householde_name,encoder_list,decoder_list):\n",
    "  att_encoder_values = interpretation['encoder_variables'].to('cpu').numpy()\n",
    "  att_decoder_values = interpretation['decoder_variables'].to('cpu').numpy()\n",
    "  tft_encoder = encoder_list\n",
    "  tft_decoder = decoder_list\n",
    "  encoder_dict = {}\n",
    "  decoder_dict = {}\n",
    "  for value, name in zip(att_encoder_values,tft_encoder):   \n",
    "    encoder_dict[name] = [f\"{np.round(value,4)*100:.2f}%\"]\n",
    "  encoder_dict['House_Hold'] = [householde_name.split(\".\")[0].upper()]\n",
    "  for value, name in zip(att_decoder_values,tft_decoder):   \n",
    "    decoder_dict[name] = [f\"{np.round(value,4)*100:.2f}%\"]\n",
    "  decoder_dict['House_Hold'] = [householde_name.split(\".\")[0].upper()]\n",
    "  return encoder_dict,decoder_dict\n",
    "\n",
    "\n",
    "def cleaning_eval_metrics_results(path_origin, path_destiny,model_name):\n",
    "    list_csv = os.listdir(path_origin)\n",
    "    df_list = []\n",
    "    for csv in list_csv:\n",
    "        if os.path.isfile(os.path.join(path_origin,csv)):\n",
    "            print(csv)\n",
    "            df_list.append(pd.read_csv(path_origin + \"/\" + csv))\n",
    "        else:\n",
    "           print('Nao e csv')\n",
    "    concat_df = pd.concat(df_list)\n",
    "    concat_df.to_csv(path_destiny + \"/\" + f\"{model_name}_metrics_results.csv\",index=False)\n",
    "\n",
    "\n",
    "def cleaning_attention_results(path_origin, path_destiny):\n",
    "    list_csv = os.listdir(path_origin)\n",
    "    df_encoder_list = []\n",
    "    df_decoder_list = []\n",
    "    for csv in list_csv:\n",
    "        if os.path.isfile(os.path.join(path_origin,csv)):\n",
    "            if csv.split(\"_\")[0] == 'decoder':\n",
    "                df_encoder_list.append(pd.read_csv(path_origin + \"/\" + csv))\n",
    "            else:\n",
    "                df_decoder_list.append(pd.read_csv(path_origin + \"/\" + csv))\n",
    "        else:\n",
    "            print('Nao e csv')\n",
    "    concat_df_encoder = pd.concat(df_encoder_list)\n",
    "    concat_df_decoder = pd.concat(df_decoder_list)\n",
    "    concat_df_encoder.to_csv(path_destiny + \"/\" + \"encoder_attention_results.csv\",index=False)\n",
    "    concat_df_decoder.to_csv(path_destiny + \"/\" + \"decoder_attention_results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(yaml_name):\n",
    "    conf = OmegaConf.load(yaml_name)\n",
    "    return conf\n",
    "\n",
    "def get_covariate_combination(path):\n",
    "    \"\"\" \n",
    "    look at eval_metric folder to get all combinations of covariates and return all combinations in a list\n",
    "    \"\"\"\n",
    "    path = config.paths.eval_folder\n",
    "    combinations = os.listdir(path)\n",
    "    feature_combination = {}\n",
    "    for combination in combinations:\n",
    "        feature_combination[combination] = combination.split('-')\n",
    "    return feature_combination\n",
    "\n",
    "\n",
    "config = get_config(\"lstm_config.yaml\")\n",
    "\n",
    "feature_combination = get_covariate_combination(config.paths.eval_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_requires_grad(model):\n",
    "    \"\"\"\n",
    "    Função auxiliar para garantir que todos os parâmetros do modelo tenham requires_grad=True\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    return model\n",
    "\n",
    "def process_batch(batch):\n",
    "    \"\"\"\n",
    "    Processa cada batch para garantir que todos os tensores tenham requires_grad=True\n",
    "    \"\"\"\n",
    "    if isinstance(batch, dict):\n",
    "        return {key: process_batch(value) for key, value in batch.items()}\n",
    "    elif isinstance(batch, list):\n",
    "        return [process_batch(item) for item in batch]\n",
    "    elif isinstance(batch, tuple):\n",
    "        return tuple(process_batch(item) for item in batch)\n",
    "    elif isinstance(batch, torch.Tensor):\n",
    "        return batch.clone().detach().requires_grad_(True)\n",
    "    else:\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RNN_model(df,\n",
    "                  csv_file_name,\n",
    "                  cell_type,\n",
    "                  path_pred,\n",
    "                  path_metrics_val,\n",
    "                  covariates,\n",
    "                  learning_rate = .1,\n",
    "                  hidden_size = 15,\n",
    "                  dropout = .2,\n",
    "                  loss = MAE(),\n",
    "                  optimizer = \"Ranger\",\n",
    "                  rnn_layers  = 2,\n",
    "                  patience=10,\n",
    "                  max_prediction_length = 168,\n",
    "                  max_encoder_length = 720,\n",
    "                  batch_size = 128,\n",
    "                  seed = 81):\n",
    "\n",
    "    df['Energy_kwh'] = df['Energy_kwh'].astype('float32')\n",
    "    df['time_idx'] = df['time_idx'].astype('int32')\n",
    "    training_cutoff = df[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "    pl.seed_everything(seed)\n",
    "\n",
    "    if covariates == [\"time_idx\"]:\n",
    "        training = TimeSeriesDataSet(\n",
    "                                df[lambda x: df.time_idx <= training_cutoff],\n",
    "                                time_idx = 'time_idx',\n",
    "                                target = 'Energy_kwh',\n",
    "                                group_ids = ['house_hold'],\n",
    "                                time_varying_known_reals=['time_idx'],\n",
    "                                time_varying_unknown_reals = ['Energy_kwh'],\n",
    "                                static_categoricals=['house_hold'],\n",
    "                                min_encoder_length = max_encoder_length // 2,\n",
    "                                max_encoder_length = max_encoder_length,\n",
    "                                min_prediction_length=1,\n",
    "                                max_prediction_length = max_prediction_length,\n",
    "                                categorical_encoders = {'house_hold': NaNLabelEncoder(add_nan=True, warn=True)}\n",
    "        )\n",
    "    else:\n",
    "        training = TimeSeriesDataSet(\n",
    "                                    df[lambda x: df.time_idx <= training_cutoff],\n",
    "                                    time_idx = 'time_idx',\n",
    "                                    target = 'Energy_kwh',\n",
    "                                    group_ids = ['house_hold'],\n",
    "                                    #time_varying_known_reals=['time_idx'],\n",
    "                                    time_varying_unknown_reals = ['Energy_kwh'],\n",
    "                                    static_categoricals=['house_hold'],\n",
    "                                    time_varying_known_categoricals = covariates,\n",
    "                                    min_encoder_length = max_encoder_length // 2,\n",
    "                                    max_encoder_length = max_encoder_length,\n",
    "                                    min_prediction_length=1,\n",
    "                                    max_prediction_length = max_prediction_length,\n",
    "                                    categorical_encoders = {'house_hold': NaNLabelEncoder(add_nan=True, warn=True)}\n",
    "        )\n",
    "\n",
    "\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, \n",
    "                                                df,\n",
    "                                                predict = True,\n",
    "                                                stop_randomization = True)\n",
    "\n",
    "\n",
    "    train_dataloader = training.to_dataloader(train = True,\n",
    "                                            batch_size = batch_size,\n",
    "                                            num_workers = 1)\n",
    "\n",
    "\n",
    "    val_dataloader = validation.to_dataloader(train = False,\n",
    "                                            batch_size = batch_size,\n",
    "                                            num_workers = 1)\n",
    "\n",
    "    rnn = RecurrentNetwork.from_dataset(\n",
    "                                            training,\n",
    "                                            cell_type =cell_type,\n",
    "                                            learning_rate = learning_rate,\n",
    "                                            hidden_size = hidden_size,\n",
    "                                            dropout = dropout,\n",
    "                                            loss = loss,\n",
    "                                            optimizer = optimizer,\n",
    "                                            rnn_layers  = rnn_layers\n",
    "        )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor = \"val_loss\",\n",
    "                                    min_delta = 0.0001,\n",
    "                                    patience = patience,\n",
    "                                    verbose = True,\n",
    "                                    mode = \"min\")\n",
    "\n",
    "\n",
    "    lr_logger = LearningRateMonitor()\n",
    "    logger_LSTM = TensorBoardLogger(f\"{cell_type}_logs\")\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\")\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "                            max_epochs = 350,\n",
    "                            accelerator = 'gpu',\n",
    "                            enable_model_summary = True,\n",
    "                            limit_train_batches = 300,\n",
    "                            gradient_clip_val = 0.1,\n",
    "                            callbacks = [lr_logger, early_stop_callback, checkpoint_callback],\n",
    "                            logger = logger_LSTM,\n",
    "                            enable_progress_bar=False\n",
    "            )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    trainer.fit(\n",
    "                    rnn,\n",
    "                    train_dataloaders = train_dataloader,\n",
    "                    val_dataloaders = val_dataloader)\n",
    "    end_time = time.time()  \n",
    "    execution_time = end_time - start_time  \n",
    "\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    best_rnn = RecurrentNetwork.load_from_checkpoint(best_model_path)\n",
    "\n",
    "    best_model_path = str(best_model_path)\n",
    "    best_rnn = RecurrentNetwork.load_from_checkpoint(best_model_path)\n",
    "\n",
    "    predictions = best_rnn.predict(val_dataloader, mode = \"raw\", return_x = True)\n",
    "\n",
    "    df_predictions = pd.DataFrame({'time_idx':predictions.x['decoder_time_idx'][0].to('cpu').numpy(),\n",
    "                                'Real':predictions.x['decoder_target'][0].to('cpu').numpy().round(3),\n",
    "                                'predict':predictions.output[0][0].to('cpu').numpy().round(3).squeeze()})\n",
    "\n",
    "    df_predictions.to_csv(path_pred + csv_file_name, index=False)\n",
    "\n",
    "    eval_dict = evaluation_metrics(val_dataloader,best_rnn,csv_file_name,execution_time)\n",
    "    df_eval_metrics = pd.DataFrame(eval_dict)\n",
    "    df_eval_metrics.to_csv(path_metrics_val + csv_file_name, index=False)\n",
    "    \n",
    "    #shutil.rmtree('lightning_logs')\n",
    "    #shutil.rmtree('LSTM_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions/dayofweek_num/stretchedSociety_block_91_MAC001165.csv\n",
      "predictions/dayofweek_num/establishedAffluence_block_25_MAC003004.csv\n",
      "predictions/hour/stretchedSociety_block_91_MAC001165.csv\n",
      "predictions/hour/establishedAffluence_block_25_MAC003004.csv\n",
      "predictions/holiday/stretchedSociety_block_91_MAC001165.csv\n",
      "predictions/holiday/establishedAffluence_block_25_MAC003004.csv\n",
      "predictions/precipType/stretchedSociety_block_91_MAC001165.csv\n",
      "predictions/precipType/establishedAffluence_block_25_MAC003004.csv\n",
      "predictions/month/stretchedSociety_block_91_MAC001165.csv\n",
      "predictions/month/establishedAffluence_block_25_MAC003004.csv\n",
      "predictions/#/stretchedSociety_block_91_MAC001165.csv\n",
      "predictions/#/establishedAffluence_block_25_MAC003004.csv\n",
      "predictions/bool_weather_missing_values/stretchedSociety_block_91_MAC001165.csv\n",
      "predictions/bool_weather_missing_values/establishedAffluence_block_25_MAC003004.csv\n"
     ]
    }
   ],
   "source": [
    "for folder_name,comb_features in feature_combination.items():\n",
    "    if comb_features[0] == \"#\":\n",
    "        comb_features = ['time_idx']\n",
    "    for csv in os.listdir(config.paths.data_dir)[:2]:\n",
    "        print(config.paths.pred_folder+folder_name+'/'+csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(config.paths.data_dir)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(config.paths.data_dir)[20:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dayofweek_num': ['dayofweek_num'],\n",
       " 'hour': ['hour'],\n",
       " 'holiday': ['holiday'],\n",
       " 'precipType': ['precipType'],\n",
       " 'month': ['month'],\n",
       " '#': ['#'],\n",
       " 'bool_weather_missing_values': ['bool_weather_missing_values']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_combination ## anterior dict problema no new Training model for holiday using stretchedSociety_block_91_MAC000350.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new =  {\n",
    " '#': ['#'],\n",
    " #'hour':['hour'],\n",
    " #'precipType': ['precipType'],\n",
    " #'month': ['month'],\n",
    " #'bool_weather_missing_values': ['bool_weather_missing_values'],\n",
    " #'holiday': ['holiday'],\n",
    " #'precipType': ['precipType']\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"stretchedSociety_block_91_MAC000350.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#': ['#']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_problematicos = [\"stretchedSociety_block_91_MAC000350.csv\",\"stretchedSociety_block_107_MAC001785.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_treinados = os.listdir(\"eval_metrics/#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for # using establishedAffluence_block_26_MAC002966.csv\n",
      "9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4050 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2024-11-10 18:55:06.568954: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-10 18:55:06.602798: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-10 18:55:07.394449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type           | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | loss             | MAE            | 0      | train\n",
      "1 | logging_metrics  | ModuleList     | 0      | train\n",
      "2 | embeddings       | MultiEmbedding | 2      | train\n",
      "3 | rnn              | LSTM           | 3.2 K  | train\n",
      "4 | output_projector | Linear         | 16     | train\n",
      "------------------------------------------------------------\n",
      "3.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n",
      "Metric val_loss improved. New best score: 0.186\n",
      "Metric val_loss improved by 0.052 >= min_delta = 0.0001. New best score: 0.134\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m168\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mrun_RNN_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_prediction_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m168\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_encoder_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m168\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcovariates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcomb_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcsv_file_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcell_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpath_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpaths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfolder_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpath_metrics_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpaths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfolder_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 108\u001b[0m, in \u001b[0;36mrun_RNN_model\u001b[0;34m(df, csv_file_name, cell_type, path_pred, path_metrics_val, covariates, learning_rate, hidden_size, dropout, loss, optimizer, rnn_layers, patience, max_prediction_length, max_encoder_length, batch_size, seed)\u001b[0m\n\u001b[1;32m     96\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     97\u001b[0m                         max_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m350\u001b[39m,\n\u001b[1;32m     98\u001b[0m                         accelerator \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m                         enable_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    105\u001b[0m         )\n\u001b[1;32m    107\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 108\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \n\u001b[1;32m    113\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time  \n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1030\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1030\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:141\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:295\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_accumulate():\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# clear gradients to not leave any unused memory during validation\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_model_zero_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39m_first_loop_iter \u001b[38;5;241m=\u001b[39m first_loop_iter\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:142\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_run_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:265\u001b[0m, in \u001b[0;36m_EvaluationLoop.on_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m     dl_outputs\u001b[38;5;241m.\u001b[39mupdate(epoch_end_logged_outputs)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# log metrics\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logger_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_eval_end_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_logged_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_evaluation_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:150\u001b[0m, in \u001b[0;36m_LoggerConnector.log_eval_end_metrics\u001b[0;34m(self, metrics)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# log all the metrics as a single dict\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:118\u001b[0m, in \u001b[0;36m_LoggerConnector.log_metrics\u001b[0;34m(self, metrics, step)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m logger \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mloggers:\n\u001b[1;32m    117\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_metrics(metrics\u001b[38;5;241m=\u001b[39mscalar_metrics, step\u001b[38;5;241m=\u001b[39mstep)\n\u001b[0;32m--> 118\u001b[0m     \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning_utilities/core/rank_zero.py:42\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `rank_zero_only.rank` needs to be set before use\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/loggers/tensorboard.py:220\u001b[0m, in \u001b[0;36mTensorBoardLogger.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m hparams_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNAME_HPARAMS_FILE)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# save the metatags file if it doesn't exist and the log directory exists\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs\u001b[38;5;241m.\u001b[39misfile(hparams_file):\n\u001b[1;32m    221\u001b[0m     save_hparams_to_yaml(hparams_file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/fabric/utilities/cloud_io.py:132\u001b[0m, in \u001b[0;36m_is_dir\u001b[0;34m(fs, path, strict)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Check if the path is not already taken by a file. If not, it is considered a valid directory-like path\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# because the directory (and all non-existing parent directories) will be created on the fly.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(path)\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fsspec/implementations/local.py:138\u001b[0m, in \u001b[0;36mLocalFileSystem.isdir\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misdir\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m--> 138\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strip_protocol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(path)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fsspec/implementations/local.py:246\u001b[0m, in \u001b[0;36mLocalFileSystem._strip_protocol\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal:\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    244\u001b[0m     path \u001b[38;5;241m=\u001b[39m path[\u001b[38;5;241m6\u001b[39m:]\n\u001b[0;32m--> 246\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mmake_path_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# This code-path is a stripped down version of\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# > drive, path = ntpath.splitdrive(path)\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# Absolute drive-letter path, e.g. X:\\Windows\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;66;03m# Relative path with drive, e.g. X:Windows\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fsspec/implementations/local.py:304\u001b[0m, in \u001b[0;36mmake_path_posix\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    303\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;66;03m# NT handling\u001b[39;00m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m path[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;66;03m# path is like \"/c:/local/path\"\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "for folder_name,comb_features in new.items():\n",
    "    if comb_features[0] == \"#\":\n",
    "        comb_features = ['time_idx']\n",
    "    for csv in os.listdir(config.paths.data_dir)[7:]:\n",
    "        if csv in csv_problematicos + csv_treinados:\n",
    "            pass\n",
    "        else:\n",
    "            df = get_csv(config.paths.data_dir + csv)\n",
    "            \n",
    "\n",
    "            print(f\"Training model for {folder_name} using {csv}\")\n",
    "            print(len(df)-168)\n",
    "            run_RNN_model(df,\n",
    "                            max_prediction_length = 168,\n",
    "                            max_encoder_length = len(df)-168,\n",
    "                            covariates = comb_features,\n",
    "                            csv_file_name = csv,\n",
    "                            cell_type = \"LSTM\",\n",
    "                            path_pred = config.paths.pred_folder + folder_name+'/',\n",
    "                            path_metrics_val=config.paths.eval_folder + folder_name+'/')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new =  {\n",
    " #'#': ['#'],\n",
    " 'precipType': ['precipType'],\n",
    " 'month': ['month'],\n",
    " 'bool_weather_missing_values': ['bool_weather_missing_values'],\n",
    " 'holiday': ['holiday'],\n",
    " 'precipType': ['precipType']\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10153 entries, 0 to 10152\n",
      "Data columns (total 17 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Unnamed: 0                   10153 non-null  int64  \n",
      " 1   time_idx                     10153 non-null  int64  \n",
      " 2   time                         10153 non-null  object \n",
      " 3   Energy_kwh                   10153 non-null  float64\n",
      " 4   house_hold                   10153 non-null  object \n",
      " 5   temperature                  10153 non-null  float64\n",
      " 6   windSpeed                    10153 non-null  float64\n",
      " 7   precipType                   10153 non-null  object \n",
      " 8   icon                         10153 non-null  object \n",
      " 9   summary                      10153 non-null  object \n",
      " 10  holiday                      10153 non-null  object \n",
      " 11  bool_weather_missing_values  10153 non-null  object \n",
      " 12  year                         10153 non-null  object \n",
      " 13  month                        10153 non-null  object \n",
      " 14  day                          10153 non-null  object \n",
      " 15  hour                         10153 non-null  object \n",
      " 16  dayofweek_num                10153 non-null  object \n",
      "dtypes: float64(3), int64(2), object(12)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "Training model for # using stretchedSociety_block_91_MAC000350.csv\n",
      "9985\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m168\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mrun_RNN_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_prediction_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m168\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_encoder_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m168\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcovariates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcomb_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcsv_file_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcell_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpath_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpaths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfolder_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpath_metrics_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpaths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfolder_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[69], line 73\u001b[0m, in \u001b[0;36mrun_RNN_model\u001b[0;34m(df, csv_file_name, cell_type, path_pred, path_metrics_val, covariates, learning_rate, hidden_size, dropout, loss, optimizer, rnn_layers, patience, max_prediction_length, max_encoder_length, batch_size, seed)\u001b[0m\n\u001b[1;32m     64\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m training\u001b[38;5;241m.\u001b[39mto_dataloader(train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     65\u001b[0m                                         batch_size \u001b[38;5;241m=\u001b[39m batch_size,\n\u001b[1;32m     66\u001b[0m                                         num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     69\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m validation\u001b[38;5;241m.\u001b[39mto_dataloader(train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     70\u001b[0m                                         batch_size \u001b[38;5;241m=\u001b[39m batch_size,\n\u001b[1;32m     71\u001b[0m                                         num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m rnn \u001b[38;5;241m=\u001b[39m \u001b[43mRecurrentNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mcell_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mrnn_layers\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrnn_layers\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m early_stop_callback \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     85\u001b[0m                                 min_delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m,\n\u001b[1;32m     86\u001b[0m                                 patience \u001b[38;5;241m=\u001b[39m patience,\n\u001b[1;32m     87\u001b[0m                                 verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m                                 mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m lr_logger \u001b[38;5;241m=\u001b[39m LearningRateMonitor()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/pytorch_forecasting/models/rnn/__init__.py:158\u001b[0m, in \u001b[0;36mRecurrentNetwork.from_dataset\u001b[0;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m new_kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdeduce_default_output_parameters(dataset\u001b[38;5;241m=\u001b[39mdataset, kwargs\u001b[38;5;241m=\u001b[39mkwargs, default_loss\u001b[38;5;241m=\u001b[39mMAE()))\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mtarget_normalizer, NaNLabelEncoder) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mtarget_normalizer, MultiNormalizer)\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m([\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(normalizer, NaNLabelEncoder) \u001b[38;5;28;01mfor\u001b[39;00m normalizer \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mtarget_normalizer])\n\u001b[1;32m    157\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget(s) should be continuous - categorical targets are not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# todo: remove this restriction\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_encoder_known_variable_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_encoder_known_variable_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/pytorch_forecasting/models/base_model.py:1742\u001b[0m, in \u001b[0;36mBaseModelWithCovariates.from_dataset\u001b[0;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   1723\u001b[0m     static_categoricals\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mstatic_categoricals,\n\u001b[1;32m   1724\u001b[0m     time_varying_categoricals_encoder\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     categorical_groups\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mvariable_groups,\n\u001b[1;32m   1740\u001b[0m )\n\u001b[1;32m   1741\u001b[0m new_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m-> 1742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/pytorch_forecasting/models/base_model.py:2069\u001b[0m, in \u001b[0;36mAutoRegressiveBaseModel.from_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m   2066\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m lag \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(lags\u001b[38;5;241m.\u001b[39mget(target, [])), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall target lags in dataset must be the same but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlags\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2068\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_lags\u001b[39m\u001b[38;5;124m\"\u001b[39m, {name: dataset\u001b[38;5;241m.\u001b[39m_get_lagged_names(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m lags})\n\u001b[0;32m-> 2069\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/pytorch_forecasting/models/base_model.py:1308\u001b[0m, in \u001b[0;36mBaseModel.from_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m   1307\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_parameters()\n\u001b[0;32m-> 1308\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mmulti_target:\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1311\u001b[0m         net\u001b[38;5;241m.\u001b[39mloss, MultiLoss\n\u001b[1;32m   1312\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple targets require loss to be MultiLoss but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnet\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/pytorch_forecasting/models/rnn/__init__.py:88\u001b[0m, in \u001b[0;36mRecurrentNetwork.__init__\u001b[0;34m(self, cell_type, hidden_size, rnn_layers, dropout, static_categoricals, static_reals, time_varying_categoricals_encoder, time_varying_categoricals_decoder, categorical_groups, time_varying_reals_encoder, time_varying_reals_decoder, embedding_sizes, embedding_paddings, embedding_labels, x_reals, x_categoricals, output_size, target, target_lags, loss, logging_metrics, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     logging_metrics \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([SMAPE(), MAE(), RMSE(), MAPE(), MASE()])\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# store loss function separately as it is a module\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(loss\u001b[38;5;241m=\u001b[39mloss, logging_metrics\u001b[38;5;241m=\u001b[39mlogging_metrics, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/core/mixins/hparams_mixin.py:130\u001b[0m, in \u001b[0;36mHyperparametersMixin.save_hyperparameters\u001b[0;34m(self, ignore, frame, logger, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current_frame:\n\u001b[1;32m    129\u001b[0m         frame \u001b[38;5;241m=\u001b[39m current_frame\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 130\u001b[0m \u001b[43msave_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgiven_hparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgiven_hparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:214\u001b[0m, in \u001b[0;36msave_hyperparameters\u001b[0;34m(obj, ignore, frame, given_hparams, *args)\u001b[0m\n\u001b[1;32m    208\u001b[0m         rank_zero_warn(\n\u001b[1;32m    209\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is an instance of `nn.Module` and is already saved during checkpointing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    210\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m It is recommended to ignore them using `self.save_hyperparameters(ignore=[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m])`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# make a deep copy so there are no other runtime changes reflected\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m obj\u001b[38;5;241m.\u001b[39m_hparams_initial \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[1;32m    296\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 297\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/torch/_tensor.py:86\u001b[0m, in \u001b[0;36mTensor.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__deepcopy__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, memo)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly Tensors created explicitly by the user \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(graph leaves) support the deepcopy protocol at the moment.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you were attempting to deepcopy a module, this may be because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof a torch.nn.utils.weight_norm usage, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msee https://github.com/pytorch/pytorch/pull/103001\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m     )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001"
     ]
    }
   ],
   "source": [
    "torch.is_grad_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-park",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
